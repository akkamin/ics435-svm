{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import Perceptron\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for plotting graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to define plots\n",
    "def define_plot(clf, X, y, title, index, dims):\n",
    "    plt.subplot(dims[0], dims[1], index)\n",
    "    plot_decision_regions(X=X.values, y=y.values, clf=clf, legend=2)\n",
    "    plt.xlabel(X.columns[0], size=14)\n",
    "    plt.ylabel(X.columns[1], size=14)\n",
    "    plt.title(title, size=16)\n",
    "    plt.axvline(x=0, color='black', linestyle='--')\n",
    "    plt.axhline(y=0, color='black', linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for labeling data and creating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data uniform \n",
    "def uniform_data(data_size):\n",
    "    return np.random.uniform(low=-5, high=5, size=data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple perceptron And with random data\n",
    "### add in noise\n",
    "def and_label_with_noise(df):\n",
    "    rtn = []\n",
    "    offset = 1\n",
    "    for i in range(df.shape[0]):\n",
    "        x = df['X'][i]\n",
    "        y = df['Y'][i]\n",
    "        if((x<offset and x>-offset) and (y<offset and y>-offset)):\n",
    "            if(random.randint(0,10) <= 5):\n",
    "                if df['X'][i] >= df['Y'][i]:\n",
    "                    rtn.append(-1)\n",
    "                else:\n",
    "                    rtn.append(1)\n",
    "            else:\n",
    "                if df['X'][i] >= df['Y'][i]:\n",
    "                    rtn.append(1)\n",
    "                else:\n",
    "                    rtn.append(-1)\n",
    "                    \n",
    "        else:\n",
    "            if df['X'][i] >= df['Y'][i]:\n",
    "                rtn.append(1)\n",
    "            else:\n",
    "                rtn.append(-1)\n",
    "    return rtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple perceptron And with random data\n",
    "def and_label(df):\n",
    "    rtn = []\n",
    "    for i in range(df.shape[0]):\n",
    "        if df['X'][i] >= df['Y'][i]:\n",
    "            rtn.append(1)\n",
    "        else:\n",
    "            rtn.append(-1)\n",
    "    return rtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xor_label(df):\n",
    "    rtn = []\n",
    "    for i in range(df.shape[0]):\n",
    "        if (df['X'][i] >= 0 and df['Y'][i] < 0) or (df['X'][i] < 0 and df['Y'][i] >= 0):\n",
    "            rtn.append(1)\n",
    "        else:\n",
    "            rtn.append(-1)\n",
    "    return rtn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing svm -c softmargin values with the linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = uniform_data(100)\n",
    "y_1 = uniform_data(100)\n",
    "\n",
    "x_2 = uniform_data(200)\n",
    "y_2 = uniform_data(200)\n",
    "\n",
    "and_df_train = pd.DataFrame({'X': x_1,\n",
    "                       'Y': y_1})\n",
    "and_df_train['Label'] = and_label_with_noise(and_df_train)\n",
    "\n",
    "X = and_df_train[['X', 'Y']]\n",
    "y = and_df_train['Label']\n",
    "\n",
    "\n",
    "and_df_test = pd.DataFrame({'X': x_2,\n",
    "                       'Y': y_2})\n",
    "and_df_test['Label'] = and_label(and_df_test)\n",
    "\n",
    "X_test = and_df_test[['X', 'Y']]\n",
    "y_test = and_df_test['Label']\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C = 1.0)\n",
    "clf.fit(X.values, y.values)\n",
    "plt.figure(figsize=(22,14))\n",
    "dims = [2,2]\n",
    "define_plot(clf, X, y, \"test1\", 1, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_1 = svm.SVC(kernel='linear', C = 1.0)\n",
    "clf_2 = svm.SVC(kernel='linear', C = 0.05)\n",
    "clf_3 = svm.SVC(kernel='linear', C = 0.01)\n",
    "clf_4 = svm.SVC(kernel='linear', C = 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_1 = cross_val_score(clf_1, X.values, y.values, cv=5)\n",
    "scores_2 = cross_val_score(clf_2, X.values, y.values, cv=5)\n",
    "scores_3 = cross_val_score(clf_3, X.values, y.values, cv=5)\n",
    "scores_4 = cross_val_score(clf_4, X.values, y.values, cv=5)\n",
    "print(scores_1)\n",
    "print(\"svm-c linear soft margin cross validation mean, 95% confidence interval\")\n",
    "print(\"linear c-soft margin value of 1.00: %0.2f, (+/- %0.2f)\" % (scores_1.mean(), scores_1.std() * 2)) \n",
    "print(\"linear c-soft margin value of 0.05: %0.2f, (+/- %0.2f)\" % (scores_2.mean(), scores_2.std() * 2)) \n",
    "print(\"linear c-soft margin value of 0.01: %0.2f, (+/- %0.2f)\" % (scores_3.mean(), scores_3.std() * 2)) \n",
    "print(\"linear c-soft margin value of 0.005: %0.2f, (+/- %0.2f)\" % (scores_4.mean(), scores_4.std() * 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_1.fit(X.values, y.values)\n",
    "clf_2.fit(X.values, y.values)\n",
    "clf_3.fit(X.values, y.values)\n",
    "clf_4.fit(X.values, y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(22,14))\n",
    "dims = [2,2]\n",
    "define_plot(clf_1, X_test, y_test, \"soft margin of 1.00 graph\", 1, dims)\n",
    "define_plot(clf_2, X_test, y_test, \"soft margin of 0.05 graph\", 2, dims)\n",
    "define_plot(clf_3, X_test, y_test, \"soft margin of 0.01 graph\", 3, dims)\n",
    "define_plot(clf_4, X_test, y_test, \"soft margin of 0.005 graph\", 4, dims)\n",
    "plt.show()\n",
    "\n",
    "print(\"soft margin of 1.00 accuracy: \" + str(clf_1.score(X_test.values, y_test.values)*100) + \" %\")\n",
    "print(\"soft margin of 0.05 accuracy: \" + str(clf_2.score(X_test.values, y_test.values)*100) + \" %\")\n",
    "print(\"soft margin of 0.01 accuracy: \" + str(clf_3.score(X_test.values, y_test.values)*100) + \" %\")\n",
    "print(\"soft margin of 0.005 accuracy: \" + str(clf_4.score(X_test.values, y_test.values)*100) + \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing svm -c softmargin values with rbf kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xor_label(df):\n",
    "    rtn = []\n",
    "    for i in range(df.shape[0]):\n",
    "        if (df['X'][i] >= 0 and df['Y'][i] < 0) or (df['X'][i] < 0 and df['Y'][i] >= 0):\n",
    "            rtn.append(1)\n",
    "        else:\n",
    "            rtn.append(-1)\n",
    "    return rtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = uniform_data(100)\n",
    "y_1 = uniform_data(100)\n",
    "\n",
    "x_2 = uniform_data(200)\n",
    "y_2 = uniform_data(200)\n",
    "\n",
    "and_df_train = pd.DataFrame({'X': x_1,\n",
    "                       'Y': y_1})\n",
    "and_df_train['Label'] = xor_label(and_df_train)\n",
    "\n",
    "X = and_df_train[['X', 'Y']]\n",
    "y = and_df_train['Label']\n",
    "\n",
    "\n",
    "and_df_test = pd.DataFrame({'X': x_2,\n",
    "                       'Y': y_2})\n",
    "and_df_test['Label'] = xor_label(and_df_test)\n",
    "\n",
    "X_test = and_df_test[['X', 'Y']]\n",
    "y_test = and_df_test['Label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_1 = svm.SVC(kernel='rbf', C = 1.0, gamma=\"auto\")\n",
    "clf_2 = svm.SVC(kernel='rbf', C = 0.7, gamma=\"auto\")\n",
    "clf_3 = svm.SVC(kernel='rbf', C = 0.4, gamma=\"auto\")\n",
    "clf_4 = svm.SVC(kernel='rbf', C = 0.1, gamma=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_1 = cross_val_score(clf_1, X.values, y.values, cv=5)\n",
    "scores_2 = cross_val_score(clf_2, X.values, y.values, cv=5)\n",
    "scores_3 = cross_val_score(clf_3, X.values, y.values, cv=5)\n",
    "scores_4 = cross_val_score(clf_4, X.values, y.values, cv=5)\n",
    "print(scores_1)\n",
    "print(\"svm-c rbf soft margin value of 1.00 cross validation mean, 95% confidence interval\")\n",
    "print(\"rbf c-soft margin value of 1.0: %0.2f, (+/- %0.2f)\" % (scores_1.mean(), scores_1.std() * 2)) \n",
    "print(\"rbf c-soft margin value of 0.8: %0.2f, (+/- %0.2f)\" % (scores_2.mean(), scores_2.std() * 2)) \n",
    "print(\"rbf c-soft margin value of 0.6: %0.2f, (+/- %0.2f)\" % (scores_3.mean(), scores_3.std() * 2)) \n",
    "print(\"rbf c-soft margin value of 0.4: %0.2f, (+/- %0.2f)\" % (scores_4.mean(), scores_4.std() * 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_1.fit(X.values, y.values)\n",
    "clf_2.fit(X.values, y.values)\n",
    "clf_3.fit(X.values, y.values)\n",
    "clf_4.fit(X.values, y.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(22,12))\n",
    "dims = [2,2]\n",
    "define_plot(clf_1, X_test, y_test, \"soft margin of 1.00 graph\", 1, dims)\n",
    "define_plot(clf_2, X_test, y_test, \"soft margin of 0.8 graph\", 2, dims)\n",
    "define_plot(clf_3, X_test, y_test, \"soft margin of 0.6 graph\", 3, dims)\n",
    "define_plot(clf_4, X_test, y_test, \"soft margin of 0.4 graph\", 4, dims)\n",
    "plt.show()\n",
    "\n",
    "print(\"soft margin of 1.00 accuracy: \" + str(clf_1.score(X_test.values, y_test.values)*100) + \" %\")\n",
    "print(\"soft margin of 0.8 accuracy: \" + str(clf_2.score(X_test.values, y_test.values)*100) + \" %\")\n",
    "print(\"soft margin of 0.6 accuracy: \" + str(clf_3.score(X_test.values, y_test.values)*100) + \" %\")\n",
    "print(\"soft margin of 0.4 accuracy: \" + str(clf_4.score(X_test.values, y_test.values)*100) + \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using cross validation to test which kernel types are best for specific data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_1 = uniform_data(100)\n",
    "y_1 = uniform_data(100)\n",
    "\n",
    "x_2 = uniform_data(200)\n",
    "y_2 = uniform_data(200)\n",
    "\n",
    "and_df_train = pd.DataFrame({'X': x_1,\n",
    "                       'Y': y_1})\n",
    "and_df_train['Label'] = and_label_with_noise(and_df_train)\n",
    "\n",
    "X = and_df_train[['X', 'Y']]\n",
    "y = and_df_train['Label']\n",
    "\n",
    "\n",
    "and_df_test = pd.DataFrame({'X': x_2,\n",
    "                       'Y': y_2})\n",
    "and_df_test['Label'] = and_label(and_df_test)\n",
    "\n",
    "X_test = and_df_test[['X', 'Y']]\n",
    "y_test = and_df_test['Label']\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C = 1.0)\n",
    "clf.fit(X.values, y.values)\n",
    "plt.figure(figsize=(22,14))\n",
    "dims = [2,2]\n",
    "define_plot(clf, X, y, \"test1\", 1, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_1 = svm.SVC(kernel='linear', C = 1.0)\n",
    "clf_2 = svm.SVC(kernel='poly', degree=2, gamma=\"auto\")\n",
    "clf_3 = svm.SVC(kernel='poly', degree=5, gamma=\"auto\")\n",
    "clf_4 = svm.SVC(kernel='poly', degree=10, gamma=\"auto\")\n",
    "clf_5 = svm.SVC(kernel='rbf', C = 1.0, gamma=\"auto\")\n",
    "clf_6 = svm.SVC(kernel='sigmoid', C = 1.0, gamma=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_1 = cross_val_score(clf_1, X.values, y.values, cv=5)\n",
    "scores_2 = cross_val_score(clf_2, X.values, y.values, cv=5)\n",
    "scores_3 = cross_val_score(clf_3, X.values, y.values, cv=5)\n",
    "scores_4 = cross_val_score(clf_4, X.values, y.values, cv=5)\n",
    "scores_5 = cross_val_score(clf_5, X.values, y.values, cv=5)\n",
    "scores_6 = cross_val_score(clf_6, X.values, y.values, cv=5)\n",
    "\n",
    "print(\"cross validation mean, 95% confidence interval\")\n",
    "print(\"linear kernel: %0.2f, (+/- %0.2f)\" % (scores_1.mean(), scores_1.std() * 2)) \n",
    "print(\"polynomial kernel deg=2: %0.2f, (+/- %0.2f)\" % (scores_2.mean(), scores_2.std() * 2)) \n",
    "print(\"polynomial kernel deg=5: %0.2f, (+/- %0.2f)\" % (scores_3.mean(), scores_3.std() * 2)) \n",
    "print(\"polynomial kernel deg=10: %0.2f, (+/- %0.2f)\" % (scores_4.mean(), scores_4.std() * 2)) \n",
    "print(\"rbf kernel: %0.2f, (+/- %0.2f)\" % (scores_5.mean(), scores_5.std() * 2)) \n",
    "print(\"sigmoid kernel: %0.2f, (+/- %0.2f)\" % (scores_6.mean(), scores_6.std() * 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_1.fit(X.values, y.values)\n",
    "clf_2.fit(X.values, y.values)\n",
    "clf_3.fit(X.values, y.values)\n",
    "clf_4.fit(X.values, y.values)\n",
    "clf_5.fit(X.values, y.values)\n",
    "clf_6.fit(X.values, y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(22,22))\n",
    "dims = [3,2]\n",
    "define_plot(clf_1, X_test, y_test, \"linear graph\", 1, dims)\n",
    "define_plot(clf_2, X_test, y_test, \"polynomial degree=2 graph\", 2, dims)\n",
    "define_plot(clf_3, X_test, y_test, \"polynomial degree=5 graph\", 3, dims)\n",
    "define_plot(clf_4, X_test, y_test, \"polynomial degree=10 graph\", 4, dims)\n",
    "define_plot(clf_5, X_test, y_test, \"rbf(gaussian) graph\", 5, dims)\n",
    "define_plot(clf_6, X_test, y_test, \"sigmoidal graph\", 6, dims)\n",
    "plt.show()\n",
    "\n",
    "print(\"linear accuracy: \" + str(clf_1.score(X_test.values, y_test.values)*100) + \" %\")\n",
    "print(\"polynomial degree=2 accuracy: \" + str(clf_2.score(X_test.values, y_test.values)*100) + \" %\")\n",
    "print(\"polynomial degree=5 accuracy: \" + str(clf_3.score(X_test.values, y_test.values)*100) + \" %\")\n",
    "print(\"polynomial degree=10 accuracy: \" + str(clf_4.score(X_test.values, y_test.values)*100) + \" %\")\n",
    "print(\"rbf accuracy: \" + str(clf_5.score(X_test.values, y_test.values)*100) + \" %\")\n",
    "print(\"sigmoidal accuracy: \" + str(clf_6.score(X_test.values, y_test.values)*100) + \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
